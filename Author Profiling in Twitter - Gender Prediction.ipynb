{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Profiling in Twitter. Gender Prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below has been used to classify tweets written in Spanish language. Statistical (such as Tf-Idf) and linguistic (such as POS Tagging) features will be used to increase the provided Baseline accuracy of 0.6643. The following code builds the best of the classifiers that has been found. Ths code does not reflect all testing that has been done under the scope of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries that will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK version:  3.2.3\n",
      "Scikit-learn version:  0.18.1\n",
      "Pandas version:  0.20.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "print(\"NLTK version: \", nltk.__version__)\n",
    "print(\"Scikit-learn version: \", sklearn.__version__)\n",
    "print(\"Pandas version: \", pandas.__version__)\n",
    "import csv\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import linear_model\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets for train and test will be loaded form a text files. Accents have been kept for Gender prediction since testing has shown the prediction is better when accents are not removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Train and Test Tweets to panda's dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_train = \"train_tweets.csv\"\n",
    "names = ['ID','T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12','T13','T14','T15','T16','T17','T18','T19','T20','T21','T22','T23','T24','T25','T26','T27','T28','T29','T30','T31','T32','T33','T34','T35','T36','T37','T38','T39','T40','T41','T42','T43','T44','T45','T46','T47','T48','T49','T50','T51','T52','T53','T54','T55','T56','T57','T58','T59','T60','T61','T62','T63','T64','T65','T66','T67','T68','T69','T70','T71','T72','T73','T74','T75','T76','T77','T78','T79','T80','T81','T82','T83','T84','T85','T86','T87','T88','T89','T90','T91','T92','T93','T94','T95','T96','T97','T98','T99','T100']\n",
    "df_train = pandas.read_csv(file_train,names=names,sep='\\t',encoding='utf-8',header=None,index_col=0,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_test = \"test_tweets.csv\"\n",
    "names = ['ID','T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12','T13','T14','T15','T16','T17','T18','T19','T20','T21','T22','T23','T24','T25','T26','T27','T28','T29','T30','T31','T32','T33','T34','T35','T36','T37','T38','T39','T40','T41','T42','T43','T44','T45','T46','T47','T48','T49','T50','T51','T52','T53','T54','T55','T56','T57','T58','T59','T60','T61','T62','T63','T64','T65','T66','T67','T68','T69','T70','T71','T72','T73','T74','T75','T76','T77','T78','T79','T80','T81','T82','T83','T84','T85','T86','T87','T88','T89','T90','T91','T92','T93','T94','T95','T96','T97','T98','T99','T100']\n",
    "df_test = pandas.read_csv(file_test,names=names,sep='\\t',encoding='utf-8',header=None,index_col=0,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing how the train and test dataframes look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>...</th>\n",
       "      <th>T91</th>\n",
       "      <th>T92</th>\n",
       "      <th>T93</th>\n",
       "      <th>T94</th>\n",
       "      <th>T95</th>\n",
       "      <th>T96</th>\n",
       "      <th>T97</th>\n",
       "      <th>T98</th>\n",
       "      <th>T99</th>\n",
       "      <th>T100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fff46823954870db83b3e6c74a60412c</th>\n",
       "      <td>Una practica en el Arbitraje CIADI es que si h...</td>\n",
       "      <td>@tromepe Esto me parece una falta de respeto p...</td>\n",
       "      <td>Muy buena carlincatura. https://t.co/jmYFN4rZe1</td>\n",
       "      <td>En estos tiempos de calor infernal, ser bomber...</td>\n",
       "      <td>Estoy cansado de leer que congreso solicita co...</td>\n",
       "      <td>@Politica_ECpe @elcomercio Sres. Tengan en cla...</td>\n",
       "      <td>@DiarioAltavoz Una historia con un mismo villa...</td>\n",
       "      <td>@DiarioAltavoz Fiscalía vs. Congreso, es una l...</td>\n",
       "      <td>@noticiAmerica Y de que manera ayudaremos si p...</td>\n",
       "      <td>@ammy_rocks24 te hablan creo Dolly. En coquete...</td>\n",
       "      <td>...</td>\n",
       "      <td>Dicen que todos los cambios son buenos, a empe...</td>\n",
       "      <td>@noticiAmerica Hubo show para toda clase de pu...</td>\n",
       "      <td>Para algunos este discurso fue mas de lo mismo...</td>\n",
       "      <td>Excelente participación la del congresista Fra...</td>\n",
       "      <td>Dios te da tres respuestas: Si, no o espera. B...</td>\n",
       "      <td>Como sea queria casarse. https://t.co/qxQxs6Jlro</td>\n",
       "      <td>El mundo ya se perdio. https://t.co/04G71M0fj5</td>\n",
       "      <td>Le quedo grande el papel de Guasón a Jared Let...</td>\n",
       "      <td>@Luisdiazc21 una caracteristica tuya mi buen a...</td>\n",
       "      <td>Macho peruano que se respeta, toma chela helad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 T1  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  Una practica en el Arbitraje CIADI es que si h...   \n",
       "\n",
       "                                                                                 T2  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @tromepe Esto me parece una falta de respeto p...   \n",
       "\n",
       "                                                                               T3  \\\n",
       "ID                                                                                  \n",
       "fff46823954870db83b3e6c74a60412c  Muy buena carlincatura. https://t.co/jmYFN4rZe1   \n",
       "\n",
       "                                                                                 T4  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  En estos tiempos de calor infernal, ser bomber...   \n",
       "\n",
       "                                                                                 T5  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  Estoy cansado de leer que congreso solicita co...   \n",
       "\n",
       "                                                                                 T6  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @Politica_ECpe @elcomercio Sres. Tengan en cla...   \n",
       "\n",
       "                                                                                 T7  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @DiarioAltavoz Una historia con un mismo villa...   \n",
       "\n",
       "                                                                                 T8  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @DiarioAltavoz Fiscalía vs. Congreso, es una l...   \n",
       "\n",
       "                                                                                 T9  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @noticiAmerica Y de que manera ayudaremos si p...   \n",
       "\n",
       "                                                                                T10  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @ammy_rocks24 te hablan creo Dolly. En coquete...   \n",
       "\n",
       "                                                        ...                          \\\n",
       "ID                                                      ...                           \n",
       "fff46823954870db83b3e6c74a60412c                        ...                           \n",
       "\n",
       "                                                                                T91  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  Dicen que todos los cambios son buenos, a empe...   \n",
       "\n",
       "                                                                                T92  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @noticiAmerica Hubo show para toda clase de pu...   \n",
       "\n",
       "                                                                                T93  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  Para algunos este discurso fue mas de lo mismo...   \n",
       "\n",
       "                                                                                T94  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  Excelente participación la del congresista Fra...   \n",
       "\n",
       "                                                                                T95  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  Dios te da tres respuestas: Si, no o espera. B...   \n",
       "\n",
       "                                                                               T96  \\\n",
       "ID                                                                                   \n",
       "fff46823954870db83b3e6c74a60412c  Como sea queria casarse. https://t.co/qxQxs6Jlro   \n",
       "\n",
       "                                                                             T97  \\\n",
       "ID                                                                                 \n",
       "fff46823954870db83b3e6c74a60412c  El mundo ya se perdio. https://t.co/04G71M0fj5   \n",
       "\n",
       "                                                                                T98  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  Le quedo grande el papel de Guasón a Jared Let...   \n",
       "\n",
       "                                                                                T99  \\\n",
       "ID                                                                                    \n",
       "fff46823954870db83b3e6c74a60412c  @Luisdiazc21 una caracteristica tuya mi buen a...   \n",
       "\n",
       "                                                                               T100  \n",
       "ID                                                                                   \n",
       "fff46823954870db83b3e6c74a60412c  Macho peruano que se respeta, toma chela helad...  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>...</th>\n",
       "      <th>T91</th>\n",
       "      <th>T92</th>\n",
       "      <th>T93</th>\n",
       "      <th>T94</th>\n",
       "      <th>T95</th>\n",
       "      <th>T96</th>\n",
       "      <th>T97</th>\n",
       "      <th>T98</th>\n",
       "      <th>T99</th>\n",
       "      <th>T100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ffb5c29c835e3cecbdaa97bfea5bbe3b</th>\n",
       "      <td>Que se haga tu voluntad y no la mía Señor... S...</td>\n",
       "      <td>A congregarse!!!LEVANTATE... DEJA LAS EXCUSAS ...</td>\n",
       "      <td>EVANGELISTA JUAN CARLOS ACEVEDO PFEIFFER https...</td>\n",
       "      <td>... La lengua, miembro pequeño del cuerpo pero...</td>\n",
       "      <td>13. Por eso en circunstancias como éstas guard...</td>\n",
       "      <td>ANTE LA IGNORANCIA, LA ESTUPIDEZ Y LA INSOLENC...</td>\n",
       "      <td>Si hay algo que nunca... jamás... never... no ...</td>\n",
       "      <td>No preguntes porqué?... Deja simplemente que e...</td>\n",
       "      <td>Mi deber está en permitir que se haga su volun...</td>\n",
       "      <td>EVANGELISTA JUAN CARLOS ACEVEDO PFEIFFER https...</td>\n",
       "      <td>...</td>\n",
       "      <td>'Can= Puedo VS. Can''t= No puedoJESUCRISTO SI ...</td>\n",
       "      <td>... En ti tengo paz!!!EVANGELISTA JUAN CARLOS ...</td>\n",
       "      <td>SOY UN GUERRERO DEL DIOS VIVIENTE Y JEHOVÁ DE ...</td>\n",
       "      <td>1. En ese momento los discípulos se acercaron ...</td>\n",
       "      <td>Cristo resucitó... Los demás \\no\\!EN QUIEN CRE...</td>\n",
       "      <td>... No cometas ese error... Te arrepentirás!!!...</td>\n",
       "      <td>13. »Por tanto —agregó Jesús—, manténganse des...</td>\n",
       "      <td>38. Den, y se les dará: se les echará en el re...</td>\n",
       "      <td>... Dar gracias al Señor en todo tiempo!!!AMÉN...</td>\n",
       "      <td>1. Señor , oye mi justo ruego; escucha mi clam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 T1  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  Que se haga tu voluntad y no la mía Señor... S...   \n",
       "\n",
       "                                                                                 T2  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  A congregarse!!!LEVANTATE... DEJA LAS EXCUSAS ...   \n",
       "\n",
       "                                                                                 T3  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  EVANGELISTA JUAN CARLOS ACEVEDO PFEIFFER https...   \n",
       "\n",
       "                                                                                 T4  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  ... La lengua, miembro pequeño del cuerpo pero...   \n",
       "\n",
       "                                                                                 T5  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  13. Por eso en circunstancias como éstas guard...   \n",
       "\n",
       "                                                                                 T6  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  ANTE LA IGNORANCIA, LA ESTUPIDEZ Y LA INSOLENC...   \n",
       "\n",
       "                                                                                 T7  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  Si hay algo que nunca... jamás... never... no ...   \n",
       "\n",
       "                                                                                 T8  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  No preguntes porqué?... Deja simplemente que e...   \n",
       "\n",
       "                                                                                 T9  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  Mi deber está en permitir que se haga su volun...   \n",
       "\n",
       "                                                                                T10  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  EVANGELISTA JUAN CARLOS ACEVEDO PFEIFFER https...   \n",
       "\n",
       "                                                        ...                          \\\n",
       "ID                                                      ...                           \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b                        ...                           \n",
       "\n",
       "                                                                                T91  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  'Can= Puedo VS. Can''t= No puedoJESUCRISTO SI ...   \n",
       "\n",
       "                                                                                T92  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  ... En ti tengo paz!!!EVANGELISTA JUAN CARLOS ...   \n",
       "\n",
       "                                                                                T93  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  SOY UN GUERRERO DEL DIOS VIVIENTE Y JEHOVÁ DE ...   \n",
       "\n",
       "                                                                                T94  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  1. En ese momento los discípulos se acercaron ...   \n",
       "\n",
       "                                                                                T95  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  Cristo resucitó... Los demás \\no\\!EN QUIEN CRE...   \n",
       "\n",
       "                                                                                T96  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  ... No cometas ese error... Te arrepentirás!!!...   \n",
       "\n",
       "                                                                                T97  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  13. »Por tanto —agregó Jesús—, manténganse des...   \n",
       "\n",
       "                                                                                T98  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  38. Den, y se les dará: se les echará en el re...   \n",
       "\n",
       "                                                                                T99  \\\n",
       "ID                                                                                    \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  ... Dar gracias al Señor en todo tiempo!!!AMÉN...   \n",
       "\n",
       "                                                                               T100  \n",
       "ID                                                                                   \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  1. Señor , oye mi justo ruego; escucha mi clam...  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Train and Test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_labels_train = \"train_classes.csv\"\n",
    "names = ['ID','Variety','Gender']\n",
    "df_train_labels = pandas.read_csv(file_labels_train,names=names,sep='\\t',encoding='utf-8',header=None,index_col=0,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_labels_test = \"test_classes.csv\"\n",
    "names = ['ID','Variety','Gender']\n",
    "df_test_labels = pandas.read_csv(file_labels_test,names=names,sep='\\t',encoding='utf-8',header=None,index_col=0,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing how the label dataframes look like. Two different labels can be found for each tweet's author, which classify it in Variety (country the author comes from) and Gender (either the author is male or female)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variety</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fff46823954870db83b3e6c74a60412c</th>\n",
       "      <td>peru</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Variety Gender\n",
       "ID                                             \n",
       "fff46823954870db83b3e6c74a60412c    peru   male"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_labels.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variety</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ffb5c29c835e3cecbdaa97bfea5bbe3b</th>\n",
       "      <td>colombia</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Variety Gender\n",
       "ID                                               \n",
       "ffb5c29c835e3cecbdaa97bfea5bbe3b  colombia   male"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labels.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets' text pre-processing. Text in tweets will be converted to lowercase, tokenized using Twitter tokenizer and spanish stop-words will be removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.applymap(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df_test.applymap(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list is built in order to being able to access tweet per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lista_tweets = ['T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12','T13','T14','T15','T16','T17','T18','T19','T20','T21','T22','T23','T24','T25','T26','T27','T28','T29','T30','T31','T32','T33','T34','T35','T36','T37','T38','T39','T40','T41','T42','T43','T44','T45','T46','T47','T48','T49','T50','T51','T52','T53','T54','T55','T56','T57','T58','T59','T60','T61','T62','T63','T64','T65','T66','T67','T68','T69','T70','T71','T72','T73','T74','T75','T76','T77','T78','T79','T80','T81','T82','T83','T84','T85','T86','T87','T88','T89','T90','T91','T92','T93','T94','T95','T96','T97','T98','T99','T100']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in lista_tweets:\n",
    "    df_train[item] = df_train.apply(lambda row: tknzr.tokenize(row[item]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in lista_tweets:\n",
    "    df_test[item] = df_test.apply(lambda row: tknzr.tokenize(row[item]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish stop-words removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in lista_tweets:\n",
    "    df_train[item] = df_train[item].apply(lambda tweet: [word for word in tweet if word not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in lista_tweets:\n",
    "    df_test[item] = df_test[item].apply(lambda tweet: [word for word in tweet if word not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packing tweets together. Tweets have to be de-tokenized before being submitted to following processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in lista_tweets:\n",
    "    df_train[item] = df_train[item].apply(lambda x: \" \".join(str(z) for z in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in lista_tweets:\n",
    "    df_test[item] = df_test[item].apply(lambda x: \" \".join(str(z) for z in x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional features creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additonal features will be created from the tweet's text. Linguistic features like Part Of Speech tags, and several statistical features such us word count, and Twitter specific items counting (hashtags, and more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tagger. Stanford POS Tagger has been run for both the Train and Test datasets. Resuts have been written to csv files, which will be loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_train_tag = \"tagged_train_filtered.csv\"\n",
    "names = ['ID','Adj','Conj','Det','Punc','Int','Noun','Pron','Adv','Prep','Verb','Date','Num']\n",
    "tag_df_train = pandas.read_csv(file_train_tag,names=names,sep='\\t',encoding='utf-8',header=None,index_col=0,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_test_tag = \"tagged_test_filtered.csv\"\n",
    "names = ['ID','Adj','Conj','Det','Punc','Int','Noun','Pron','Adv','Prep','Verb','Date','Num']\n",
    "tag_df_test = pandas.read_csv(file_test_tag,names=names,sep='\\t',encoding='utf-8',header=None,index_col=0,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tag's values will be standarized in order to get values in range [0..1], which are optimal for machine learning methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Adj\"]=tag_df_train[\"Adj\"]/max(tag_df_train[\"Adj\"])\n",
    "tag_df_train[\"Conj\"]=tag_df_train[\"Conj\"]/max(tag_df_train[\"Conj\"])\n",
    "tag_df_train[\"Det\"]=tag_df_train[\"Det\"]/max(tag_df_train[\"Det\"])\n",
    "tag_df_train[\"Punc\"]=tag_df_train[\"Punc\"]/max(tag_df_train[\"Punc\"])\n",
    "tag_df_train[\"Int\"]=tag_df_train[\"Int\"]/max(tag_df_train[\"Int\"])\n",
    "tag_df_train[\"Noun\"]=tag_df_train[\"Noun\"]/max(tag_df_train[\"Noun\"])\n",
    "tag_df_train[\"Pron\"]=tag_df_train[\"Pron\"]/max(tag_df_train[\"Pron\"])\n",
    "tag_df_train[\"Adv\"]=tag_df_train[\"Adv\"]/max(tag_df_train[\"Adv\"])\n",
    "tag_df_train[\"Prep\"]=tag_df_train[\"Prep\"]/max(tag_df_train[\"Prep\"])\n",
    "tag_df_train[\"Verb\"]=tag_df_train[\"Verb\"]/max(tag_df_train[\"Verb\"])\n",
    "tag_df_train[\"Date\"]=tag_df_train[\"Date\"]/max(tag_df_train[\"Date\"])\n",
    "tag_df_train[\"Num\"]=tag_df_train[\"Num\"]/max(tag_df_train[\"Num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_test[\"Adj\"]=tag_df_test[\"Adj\"]/max(tag_df_test[\"Adj\"])\n",
    "tag_df_test[\"Conj\"]=tag_df_test[\"Conj\"]/max(tag_df_test[\"Conj\"])\n",
    "tag_df_test[\"Det\"]=tag_df_test[\"Det\"]/max(tag_df_test[\"Det\"])\n",
    "tag_df_test[\"Punc\"]=tag_df_test[\"Punc\"]/max(tag_df_test[\"Punc\"])\n",
    "tag_df_test[\"Int\"]=tag_df_test[\"Int\"]/max(tag_df_test[\"Int\"])\n",
    "tag_df_test[\"Noun\"]=tag_df_test[\"Noun\"]/max(tag_df_test[\"Noun\"])\n",
    "tag_df_test[\"Pron\"]=tag_df_test[\"Pron\"]/max(tag_df_test[\"Pron\"])\n",
    "tag_df_test[\"Adv\"]=tag_df_test[\"Adv\"]/max(tag_df_test[\"Adv\"])\n",
    "tag_df_test[\"Prep\"]=tag_df_test[\"Prep\"]/max(tag_df_test[\"Prep\"])\n",
    "tag_df_test[\"Verb\"]=tag_df_test[\"Verb\"]/max(tag_df_test[\"Verb\"])\n",
    "tag_df_test[\"Date\"]=tag_df_test[\"Date\"]/max(tag_df_test[\"Date\"])\n",
    "tag_df_test[\"Num\"]=tag_df_test[\"Num\"]/max(tag_df_test[\"Num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the boxes below several tags can be selected to be removed form the POS tags feature matrix, so they will not be taken into account when building the classifier. During the testing process this has been useful to find the best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(tag_df_train[\"Conj\"])\n",
    "#del(tag_df_train[\"Det\"])\n",
    "#del(tag_df_train[\"Punc\"])\n",
    "#del(tag_df_train[\"Int\"])\n",
    "#del(tag_df_train[\"Noun\"])\n",
    "#del(tag_df_train[\"Pron\"])\n",
    "#del(tag_df_train[\"Adv\"])\n",
    "#del(tag_df_train[\"Prep\"])\n",
    "#del(tag_df_train[\"Verb\"])\n",
    "#del(tag_df_train[\"Date\"])\n",
    "#del(tag_df_train[\"Num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(tag_df_test[\"Conj\"])\n",
    "#del(tag_df_test[\"Det\"])\n",
    "#del(tag_df_test[\"Punc\"])\n",
    "#del(tag_df_test[\"Int\"])\n",
    "#del(tag_df_test[\"Noun\"])\n",
    "#del(tag_df_test[\"Pron\"])\n",
    "#del(tag_df_test[\"Adv\"])\n",
    "#del(tag_df_test[\"Prep\"])\n",
    "#del(tag_df_test[\"Verb\"])\n",
    "#del(tag_df_test[\"Date\"])\n",
    "#del(tag_df_test[\"Num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter specific features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Counting emojis per author. The Tweet's text is gone over to count how many emojis every author uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emojis_tweets_train = []\n",
    "for i in range(len(df_train)):\n",
    "    aux = df_train.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r'[\\U0001f600-\\U0001f650]', aux2)))\n",
    "    emojis_tweets_train.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emojis_tweets_test = []\n",
    "for i in range(len(df_test)):\n",
    "    aux = df_test.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r'[\\U0001f600-\\U0001f650]', aux2)))\n",
    "    emojis_tweets_test.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Emojis\"]=emojis_tweets_train\n",
    "tag_df_test[\"Emojis\"]=emojis_tweets_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, the counts are standarized [0..1] by dividing by the max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Emojis\"] = tag_df_train[\"Emojis\"]/max(tag_df_train[\"Emojis\"])\n",
    "tag_df_test[\"Emojis\"] = tag_df_test[\"Emojis\"]/max(tag_df_test[\"Emojis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Hashtags counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtags_tweets_train = []\n",
    "for i in range(len(df_train)):\n",
    "    aux = df_train.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", aux2)))\n",
    "    hashtags_tweets_train.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtags_tweets_test = []\n",
    "for i in range(len(df_test)):\n",
    "    aux = df_test.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", aux2)))\n",
    "    hashtags_tweets_test.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Hashtags\"]=hashtags_tweets_train\n",
    "tag_df_test[\"Hashtags\"]=hashtags_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Hashtags\"] = tag_df_train[\"Hashtags\"]/max(tag_df_train[\"Hashtags\"])\n",
    "tag_df_test[\"Hashtags\"] = tag_df_test[\"Hashtags\"]/max(tag_df_test[\"Hashtags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentions counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mentions_tweets_train = []\n",
    "for i in range(len(df_train)):\n",
    "    aux = df_train.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r'(?:@[\\w_]+)', aux2)))\n",
    "    mentions_tweets_train.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mentions_tweets_test = []\n",
    "for i in range(len(df_test)):\n",
    "    aux = df_test.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r'(?:@[\\w_]+)', aux2)))\n",
    "    mentions_tweets_test.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Mentions\"]=mentions_tweets_train\n",
    "tag_df_test[\"Mentions\"]=mentions_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Mentions\"] = tag_df_train[\"Mentions\"]/max(tag_df_train[\"Mentions\"])\n",
    "tag_df_test[\"Mentions\"] = tag_df_test[\"Mentions\"]/max(tag_df_test[\"Mentions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URLs counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls_tweets_train = []\n",
    "for i in range(len(df_train)):\n",
    "    aux = df_train.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', aux2)))\n",
    "    urls_tweets_train.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls_tweets_test = []\n",
    "for i in range(len(df_test)):\n",
    "    aux = df_test.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', aux2)))\n",
    "    urls_tweets_test.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"URLs\"]=urls_tweets_train\n",
    "tag_df_test[\"URLs\"]=urls_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"URLs\"] = tag_df_train[\"URLs\"]/max(tag_df_train[\"URLs\"])\n",
    "tag_df_test[\"URLs\"] = tag_df_test[\"URLs\"]/max(tag_df_test[\"URLs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting retwitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rts_tweets_train = []\n",
    "for i in range(len(df_train)):\n",
    "    aux = df_train.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", aux2)))\n",
    "    rts_tweets_train.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rts_tweets_test = []\n",
    "for i in range(len(df_test)):\n",
    "    aux = df_test.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        cuenta = cuenta + (len(re.findall(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", aux2)))\n",
    "    rts_tweets_test.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"RTs\"]=rts_tweets_train\n",
    "tag_df_test[\"RTs\"]=rts_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"RTs\"] = tag_df_train[\"RTs\"]/max(tag_df_train[\"RTs\"])\n",
    "tag_df_test[\"RTs\"] = tag_df_test[\"RTs\"]/max(tag_df_test[\"RTs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj</th>\n",
       "      <th>Conj</th>\n",
       "      <th>Det</th>\n",
       "      <th>Punc</th>\n",
       "      <th>Int</th>\n",
       "      <th>Noun</th>\n",
       "      <th>Pron</th>\n",
       "      <th>Adv</th>\n",
       "      <th>Prep</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Date</th>\n",
       "      <th>Num</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>URLs</th>\n",
       "      <th>RTs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fff46823954870db83b3e6c74a60412c</th>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.446512</td>\n",
       "      <td>0.537572</td>\n",
       "      <td>0.273247</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.356643</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.420925</td>\n",
       "      <td>0.499058</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.018373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffebe1735cd1f0e69d8210376a9dc377</th>\n",
       "      <td>0.322835</td>\n",
       "      <td>0.204651</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>0.284692</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.369584</td>\n",
       "      <td>0.188811</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>0.559611</td>\n",
       "      <td>0.310734</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.026247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149351</td>\n",
       "      <td>0.033403</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffe2faddf807b45ade8d8032ab7487d4</th>\n",
       "      <td>0.326772</td>\n",
       "      <td>0.120930</td>\n",
       "      <td>0.320809</td>\n",
       "      <td>0.138770</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.369584</td>\n",
       "      <td>0.108392</td>\n",
       "      <td>0.095455</td>\n",
       "      <td>0.416058</td>\n",
       "      <td>0.259887</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058442</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffc9c0b137c5bbb3f9173e7af991e122</th>\n",
       "      <td>0.248031</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.280347</td>\n",
       "      <td>0.211731</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.361088</td>\n",
       "      <td>0.300699</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.218978</td>\n",
       "      <td>0.367232</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.041995</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.728601</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff91e621bd80e9c980a6e7f8550a1d80</th>\n",
       "      <td>0.464567</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.595376</td>\n",
       "      <td>0.472103</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.356839</td>\n",
       "      <td>0.524476</td>\n",
       "      <td>0.718182</td>\n",
       "      <td>0.547445</td>\n",
       "      <td>0.580038</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.049869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Adj      Conj       Det      Punc  \\\n",
       "ID                                                                         \n",
       "fff46823954870db83b3e6c74a60412c  0.338583  0.446512  0.537572  0.273247   \n",
       "ffebe1735cd1f0e69d8210376a9dc377  0.322835  0.204651  0.416185  0.284692   \n",
       "ffe2faddf807b45ade8d8032ab7487d4  0.326772  0.120930  0.320809  0.138770   \n",
       "ffc9c0b137c5bbb3f9173e7af991e122  0.248031  0.186047  0.280347  0.211731   \n",
       "ff91e621bd80e9c980a6e7f8550a1d80  0.464567  0.534884  0.595376  0.472103   \n",
       "\n",
       "                                       Int      Noun      Pron       Adv  \\\n",
       "ID                                                                         \n",
       "fff46823954870db83b3e6c74a60412c  0.265306  0.345794  0.356643  0.295455   \n",
       "ffebe1735cd1f0e69d8210376a9dc377  0.102041  0.369584  0.188811  0.245455   \n",
       "ffe2faddf807b45ade8d8032ab7487d4  0.010204  0.369584  0.108392  0.095455   \n",
       "ffc9c0b137c5bbb3f9173e7af991e122  0.102041  0.361088  0.300699  0.381818   \n",
       "ff91e621bd80e9c980a6e7f8550a1d80  0.173469  0.356839  0.524476  0.718182   \n",
       "\n",
       "                                      Prep      Verb      Date       Num  \\\n",
       "ID                                                                         \n",
       "fff46823954870db83b3e6c74a60412c  0.420925  0.499058  0.072917  0.018373   \n",
       "ffebe1735cd1f0e69d8210376a9dc377  0.559611  0.310734  0.114583  0.026247   \n",
       "ffe2faddf807b45ade8d8032ab7487d4  0.416058  0.259887  0.052083  0.023622   \n",
       "ffc9c0b137c5bbb3f9173e7af991e122  0.218978  0.367232  0.239583  0.041995   \n",
       "ff91e621bd80e9c980a6e7f8550a1d80  0.547445  0.580038  0.072917  0.049869   \n",
       "\n",
       "                                    Emojis  Hashtags  Mentions      URLs  RTs  \n",
       "ID                                                                             \n",
       "fff46823954870db83b3e6c74a60412c  0.000000  0.021645  0.164927  0.161616  0.0  \n",
       "ffebe1735cd1f0e69d8210376a9dc377  0.000000  0.149351  0.033403  0.515152  0.0  \n",
       "ffe2faddf807b45ade8d8032ab7487d4  0.000000  0.058442  0.002088  0.898990  0.0  \n",
       "ffc9c0b137c5bbb3f9173e7af991e122  0.050505  0.025974  0.728601  0.348485  0.0  \n",
       "ff91e621bd80e9c980a6e7f8550a1d80  0.000000  0.019481  0.108559  0.060606  0.0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count per user. This is a linguistic feature that will be added to the feature's dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_tweets_train = []\n",
    "for i in range(len(df_train)):\n",
    "    aux = df_train.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        pals = tknzr.tokenize(aux2)\n",
    "        cuenta = cuenta + len(pals)\n",
    "    words_tweets_train.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_tweets_test = []\n",
    "for i in range(len(df_test)):\n",
    "    aux = df_test.iloc[i]\n",
    "    cuenta = 0\n",
    "    for item in lista_tweets:\n",
    "        aux2= aux[item]\n",
    "        pals = tknzr.tokenize(aux2)\n",
    "        cuenta = cuenta + len(pals)\n",
    "    words_tweets_test.append(cuenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Words\"]=words_tweets_train\n",
    "tag_df_test[\"Words\"]=words_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_df_train[\"Words\"] = tag_df_train[\"Words\"]/max(tag_df_train[\"Words\"])\n",
    "tag_df_test[\"Words\"] = tag_df_test[\"Words\"]/max(tag_df_test[\"Words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading labels for Train and Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_gender_train = df_train_labels[\"Gender\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_gender_test = df_test_labels[\"Gender\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All 100 Tweets per author have to be joined in a single string in order to use Tf-Idf Vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping all tweets per author (train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_train = []\n",
    "for i in range(len(df_train)):\n",
    "    aux = df_train.iloc[i].values.tolist()\n",
    "    tweets_train.append(' '.join(aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_test = []\n",
    "for i in range(len(df_test)):\n",
    "    aux = df_test.iloc[i].values.tolist()\n",
    "    tweets_test.append(' '.join(aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### During the testing it has been found that setting max_df parametre to 0.90 provides the best accuracy. max_df sets the maximum word fequency to be kept. That means that words with a frequency higher than 0.90 will be considered as being corpus specific stop-words and therefore will be removed since they do not contribute positively to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Unigrams + corpus stop-words removal (max_df=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfIdf_uni_df=TfidfVectorizer(analyzer=\"word\",lowercase=False, ngram_range=(1,1), max_df=0.90, min_df=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tfidf_uni_df = TfIdf_uni_df.fit_transform(tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tfidf_uni_df = TfIdf_uni_df.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Tf-Idf + features dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features dataframe is loaded as a matrix, both for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_feat_to_matrix = tag_df_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_feat_to_matrix = tag_df_test.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse matrices obtained as a result of Tf-Idf are joint with features matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dades_train_tf =hstack([train_tfidf_uni_df, df_train_feat_to_matrix]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dades_test_tf =hstack([test_tfidf_uni_df, df_test_feat_to_matrix]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_gen = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,), random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the train dataset, including Tf-Idf and created features, to the train labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_gen.fit(dades_train_tf, label_gender_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset's labels prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_ann_gen = ann_gen.predict(dades_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating accuracy and f1-score measures for the model using scikitlearn's metrics package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_tfidf_gen_feat_ann = accuracy_score(label_gender_test, pred_ann_gen)\n",
    "f1_tfidf_gen_feat_ann = metrics.f1_score(label_gender_test, pred_ann_gen, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Gender Accuracy amb features + Tf-Idf=  0.774285714286\n",
      "ANN Gender f1-score amb features + Tf-Idf=  0.772670748727\n"
     ]
    }
   ],
   "source": [
    "print (\"ANN Gender Accuracy amb features + Tf-Idf= \",str(acc_tfidf_gen_feat_ann))\n",
    "print (\"ANN Gender f1-score amb features + Tf-Idf= \",str(f1_tfidf_gen_feat_ann))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
